# 1. SORT ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•œ ë‹¤ì¤‘ ê°ì²´ì¶”ì ê¸° êµ¬í˜„

ğŸ“¢ ì„¤ëª…

* ì´ ì‹¤ìŠµì—ì„œëŠ” SORT ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ì—ì„œ ë‹¤ì¤‘ê°ì²´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

* ì´ë¥¼í†µí•´ ê°ì²´ì¶”ì ì˜ ê¸°ë³¸ê°œë…ê³¼ SORT ì•Œê³ ë¦¬ì¦˜ì˜ ì ìš©ë°©ë²•ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ“– ìš”êµ¬ì‚¬í•­

* ê°ì²´ ê²€ì¶œê¸° êµ¬í˜„ : YOLOv4ì™€ ê°™ì€ ì‚¬ì „ í›ˆë ¨ ëœ ê°ì²´ ê²€ì¶œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê° í”„ë ˆì„ì—ì„œ ê°ì²´ë¥¼ ê²€ì¶œí•©ë‹ˆë‹¤.

* mathworks.com SORT ì¶”ì ê¸° ì´ˆê¸°í™” : ê²€ì¶œëœ ê°ì²´ì˜ ê²½ê³„ìƒìë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ SORT ì¶”ì ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

* ê°ì²´ì¶”ì  : ê° í”„ë ˆì„ë§ˆë‹¤ ê²€ì¶œëœ ê°ì²´ì™€ ê¸°ì¡´ ì¶”ì ê°ì²´ë¥¼ ì—°ê´€ì‹œì¼œ ì¶”ì ì„ ìœ ì§€í•©ë‹ˆë‹¤.

* ê²°ê³¼ì‹œê°í™” : ì¶”ì ëœ ê° ê°ì²´ì— ê³ ìœ IDë¥¼ ë¶€ì—¬í•˜ê³ , í•´ë‹¹IDì™€ ê²½ê³„ìƒìë¥¼ ë¹„ë””ì˜¤í”„ë ˆì„ì— í‘œì‹œí•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.


### ê°ì²´ ê²€ì¶œê¸° êµ¬í˜„
```python
def construct_yolo_v4(cfg_path='yolov4.cfg', weights_path='yolov4.weights', names_path='coco_names.txt'):
    # í´ë˜ìŠ¤ ì´ë¦„ ë¡œë”©
    with open(names_path, 'r') as f:
        class_names = [line.strip() for line in f.readlines()]
    
    # YOLOv4 ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    model = cv.dnn.readNet(weights_path, cfg_path)

    # ì¶œë ¥ ë ˆì´ì–´ ì¶”ì¶œ
    layer_names = model.getLayerNames()
    out_layers = [layer_names[i - 1] for i in model.getUnconnectedOutLayers()]
    
    return model, out_layers, class_names

def yolo_detect(img, yolo_model, out_layers, conf_threshold=0.5, nms_threshold=0.4):
    height, width = img.shape[:2]
    
    # YOLO ì…ë ¥ í¬ë§·(blob) ìƒì„±
    blob = cv.dnn.blobFromImage(img, 1/255.0, (416, 416), swapRB=True, crop=False)
    yolo_model.setInput(blob)

    # YOLO ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
    outputs = yolo_model.forward(out_layers)

    boxes, confidences, class_ids = [], [], []

    # ì¶”ë¡ ëœ ê²°ê³¼ í•´ì„
    for output in outputs:
        for detection in output:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > conf_threshold:
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)
                boxes.append([x, y, x + w, y + h])  # ì¢Œìƒë‹¨(x,y) ~ ìš°í•˜ë‹¨(x+w, y+h)
                confidences.append(float(confidence))
                class_ids.append(class_id)

    # ë¹„ìµœëŒ€ ì–µì œ(NMS) ì ìš©í•˜ì—¬ ì¤‘ë³µ ì œê±°
    indices = cv.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)
    result = [boxes[i] + [confidences[i]] + [class_ids[i]] for i in range(len(boxes)) if i in indices]
    return result

```

* YOLOv4 ëª¨ë¸ê³¼ ê´€ë ¨ëœ ì„¤ì • íŒŒì¼(.cfg), ê°€ì¤‘ì¹˜ íŒŒì¼(.weights), í´ë˜ìŠ¤ ì´ë¦„ íŒŒì¼(.txt)ì„ ë¡œë”©í•˜ì—¬ ëª¨ë¸ì„ ì´ˆê¸°í™”

* model: YOLOv4 ëª¨ë¸ ê°ì²´

* out_layers: ì¶œë ¥ ë ˆì´ì–´ ì´ë¦„ ë¦¬ìŠ¤íŠ¸

* class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: person, car ë“±)

* Confidenceê°€ ë†’ì€ ë°•ìŠ¤ë§Œ í•„í„°ë§í•˜ê³ , ì¤‘ë³µëœ ë°•ìŠ¤ëŠ” ì œê±°(NMS).

* ìµœì¢…ì ìœ¼ë¡œ [x1, y1, x2, y2, confidence, class_id] í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.


### SORT ì¶”ì ê¸° ì´ˆê¸°í™”
```python
from sort.sort import Sort

# ì¶”ì ê¸° ê°ì²´ ìƒì„±
sort = Sort()
```

* githubì—ì„œ sort íŒŒì¼ì„ ë‹¤ìš´ë°›ê³  ì´ˆê¸°í™”í•¨


### ê°ì²´ ì¶”ì 
```python
# ê°ì²´ ê²€ì¶œ
detections = yolo_detect(frame, model, out_layers)

# ì‚¬ëŒë§Œ ì¶”ì¶œ (COCO í´ë˜ìŠ¤ì—ì„œ ì‚¬ëŒ class_id == 0)
persons = [det for det in detections if det[5] == 0]

# ì¶”ì ê¸° ì—…ë°ì´íŠ¸
if persons:
    tracks = sort.update(np.array(persons))  # í˜•ì‹: [x1, y1, x2, y2, track_id]
else:
    tracks = sort.update()  # ì¶”ì  ìœ ì§€
```

* íƒì§€ ê²°ê³¼ ì¤‘ ì‚¬ëŒë§Œ ì¶”ì¶œí•˜ì—¬ SORTì— ì „ë‹¬ (class_id == 0ì€ COCOì—ì„œ "person")

* sort.update()ë¥¼ í†µí•´ ìƒˆë¡œìš´ ë°”ìš´ë”© ë°•ìŠ¤ì— IDë¥¼ ì—°ê²°

* track_idëŠ” SORTê°€ ë¶€ì—¬í•˜ëŠ” ê³ ìœ í•œ ì¶”ì  IDë¡œ, ê°™ì€ ì‚¬ëŒì„ ì—¬ëŸ¬ í”„ë ˆì„ ë™ì•ˆ ê³„ì† ì¶”ì 

### ê²°ê³¼ ì‹œê°í™”
```python
for track in tracks:
    x1, y1, x2, y2, track_id = track.astype(int)
    color = colors[track_id % 100]  # IDë§ˆë‹¤ ìƒ‰ ë‹¤ë¥´ê²Œ
    cv.rectangle(frame, (x1, y1), (x2, y2), color, 2)
    cv.putText(frame, f'ID {track_id}', (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
```

* ê° ê°ì²´ì˜ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ ìƒ‰ìƒìœ¼ë¡œ ê·¸ë¦¼

* ê°ì²´ IDë¥¼ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œí•˜ì—¬ ì¶”ì ì´ ì˜ ë˜ê³  ìˆëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆê²Œ í•¨

* track_id % 100ì„ í†µí•´ ìµœëŒ€ 100ê°œì˜ ìƒ‰ìƒì„ ìˆœí™˜í•˜ì—¬ ê° ê°ì²´ë¥¼ êµ¬ë¶„


### ì „ì²´ì½”ë“œ
```python
import numpy as np
import cv2 as cv
import sys
from sort.sort import Sort

def construct_yolo_v4():
    # COCO í´ë˜ìŠ¤ ì´ë¦„ ë¡œë”©
    with open('coco_names.txt', 'r') as f:
        class_names = [line.strip() for line in f.readlines()]
    
    # YOLOv4 ëª¨ë¸ ë¡œë”©
    model = cv.dnn.readNet('yolov4.weights', 'yolov4.cfg')
    layer_names = model.getLayerNames()
    out_layers = [layer_names[i - 1] for i in model.getUnconnectedOutLayers()]
    
    return model, out_layers, class_names

def yolo_detect(img, yolo_model, out_layers):
    height, width = img.shape[:2]
    
    # YOLOv4ëŠ” ë³´í†µ 416x416 ì…ë ¥ì„ ì‚¬ìš©
    blob = cv.dnn.blobFromImage(img, 1/255.0, (416, 416), swapRB=True, crop=False)
    yolo_model.setInput(blob)
    outputs = yolo_model.forward(out_layers)
    
    boxes, confidences, class_ids = [], [], []
    for output in outputs:
        for detection in output:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5:
                center_x, center_y = int(detection[0] * width), int(detection[1] * height)
                w, h = int(detection[2] * width), int(detection[3] * height)
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)
                boxes.append([x, y, x + w, y + h])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    indices = cv.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)
    result = [boxes[i] + [confidences[i]] + [class_ids[i]] for i in range(len(boxes)) if i in indices]
    return result

# ëª¨ë¸ ì´ˆê¸°í™”
model, out_layers, class_names = construct_yolo_v4()
colors = np.random.uniform(0, 255, size=(100, 3))
sort = Sort()

# ì›¹ìº  ì—°ê²°
cap = cv.VideoCapture(0, cv.CAP_DSHOW)
if not cap.isOpened():
    sys.exit('ì¹´ë©”ë¼ ì—°ê²° ì‹¤íŒ¨')

while True:
    ret, frame = cap.read()
    if not ret:
        sys.exit('í”„ë ˆì„ íšë“ ì‹¤íŒ¨. ë£¨í”„ ì¢…ë£Œ.')

    # ê°ì²´ ê²€ì¶œ
    detections = yolo_detect(frame, model, out_layers)
    # ì‚¬ëŒ í´ë˜ìŠ¤ë§Œ í•„í„°ë§ (COCOì—ì„œ 0ë²ˆ classê°€ "person")
    persons = [det for det in detections if det[5] == 0]

    # ê°ì²´ ì¶”ì  ì—…ë°ì´íŠ¸
    if persons:
        tracks = sort.update(np.array(persons))
    else:
        tracks = sort.update()

    # ì‹œê°í™”
    for track in tracks:
        x1, y1, x2, y2, track_id = track.astype(int)
        color = colors[track_id % 100]
        cv.rectangle(frame, (x1, y1), (x2, y2), color, 2)
        cv.putText(frame, f'ID {track_id}', (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)

    # ì¶œë ¥
    cv.imshow('YOLOv4 + SORT Tracking', frame)
    if cv.waitKey(1) == ord('q'):
        break

cap.release()
cv.destroyAllWindows()
```

### ì‹¤í–‰ ê²°ê³¼


# 2. Mediapipeë¥¼ í™œìš©í•œ ì–¼êµ´ ëœë“œë§ˆí¬ ì¶”ì¶œ ë° ì‹œê°í™”

ğŸ“¢ ì„¤ëª…

* Mediapipeì˜ FaceMesh ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ì–¼êµ´ì˜ 468ê°œ ëœë“œë§ˆí¬ë¥¼ ì¶”ì¶œ, ì‹¤ì‹œê°„ ì˜ìƒì— ì‹œê°í™”í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ êµ¬í˜„

ğŸ“– ìš”êµ¬ì‚¬í•­

* Mediapipeì˜ FaceMesh ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ì–¼êµ´ëœë“œë§ˆí¬ ê²€ì¶œê¸°ë¥¼ ì´ˆê¸°í™”

* OpenCVë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ìº ìœ¼ë¡œë¶€í„° ì‹¤ì‹œê°„ì˜ìƒì„ ìº¡ì²˜

* ê²€ì¶œëœ ì–¼êµ´ëœë“œë§ˆí¬ë¥¼ ì‹¤ì‹œê°„ ì˜ìƒì— ì ìœ¼ë¡œ í‘œì‹œ

* ESC í‚¤ë¥¼ëˆ„ë¥´ë©´ í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë˜ë„ë¡ ì„¤ì •


### Mediapipeì˜ FaceMesh ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ì–¼êµ´ëœë“œë§ˆí¬ ê²€ì¶œê¸°ë¥¼ ì´ˆê¸°í™”
```python
import mediapipe as mp

mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh(
    static_image_mode=False,
    max_num_faces=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)
```
* mediapipe.solutions.face_mesh: Mediapipeì—ì„œ FaceMesh ê¸°ëŠ¥ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ ëª¨ë“ˆ

* static_image_mode=False: ì‹¤ì‹œê°„ ì˜ìƒì´ë¯€ë¡œ ê° í”„ë ˆì„ì—ì„œ ì¶”ì ì„ ìœ ì§€

* max_num_faces=1: í•œ ë²ˆì— ì¶”ì í•  ì–¼êµ´ì€ 1ëª…ìœ¼ë¡œ ì œí•œ

* min_detection_confidence=0.5: ì–¼êµ´ì´ ê²€ì¶œë˜ì—ˆë‹¤ê³  íŒë‹¨í•˜ê¸° ìœ„í•œ ìµœì†Œ í™•ì‹  ê°’

* min_tracking_confidence=0.5: ì¶”ì ì„ ê³„ì†í•˜ê¸° ìœ„í•œ ìµœì†Œ í™•ì‹  ê°’.

### OpenCVë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ìº ìœ¼ë¡œë¶€í„° ì‹¤ì‹œê°„ ì˜ìƒì„ ìº¡ì²˜
```python
import cv2 as cv

cap = cv.VideoCapture(0, cv.CAP_DSHOW)

while True:
    ret, frame = cap.read()
    if not ret:
        print("í”„ë ˆì„ íšë“ ì‹¤íŒ¨")
        break

```
* cv.VideoCapture(0): ë””í´íŠ¸ ì›¹ìº ì„ ì—´ì–´ ì˜ìƒ ìŠ¤íŠ¸ë¦¼ì„ ê°€ì ¸ì˜´

* cv.CAP_DSHOW: ìœˆë„ìš°ì—ì„œ DirectShow ë°©ì‹ìœ¼ë¡œ ì—°ê²°

* .read(): í˜„ì¬ í”„ë ˆì„ì„ ìº¡ì²˜ (ì„±ê³µ ì—¬ë¶€ëŠ” ret, í”„ë ˆì„ ìì²´ëŠ” frame)

* if not ret: í”„ë ˆì„ì„ ì œëŒ€ë¡œ ëª» ê°€ì ¸ì™”ì„ ê²½ìš° ë£¨í”„ë¥¼ ì¢…ë£Œ

### ê²€ì¶œëœ ì–¼êµ´ ëœë“œë§ˆí¬ë¥¼ ì‹¤ì‹œê°„ ì˜ìƒì— ì ìœ¼ë¡œ í‘œì‹œ
```python
rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)
result = face_mesh.process(rgb_frame)

if result.multi_face_landmarks:
    for face_landmarks in result.multi_face_landmarks:
        for landmark in face_landmarks.landmark:
            x = int(landmark.x * frame.shape[1])
            y = int(landmark.y * frame.shape[0])
            cv.circle(frame, (x, y), 1, (0, 255, 0), -1)
```
* MediapipeëŠ” RGB ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê¸° ë•Œë¬¸ì— OpenCVì˜ BGR ì´ë¯¸ì§€ë¥¼ ë³€í™˜

* face_mesh.process(...): í˜„ì¬ í”„ë ˆì„ì—ì„œ ì–¼êµ´ì„ ê°ì§€í•˜ê³ , ëœë“œë§ˆí¬ë¥¼ ë°˜í™˜

* result.multi_face_landmarks: ì–¼êµ´ì´ ê²€ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸

* landmark.x, landmark.y: ê° í¬ì¸íŠ¸ëŠ” ë¹„ìœ¨(0~1) í˜•íƒœë¡œ ë°˜í™˜ë˜ë¯€ë¡œ, í™”ë©´ í¬ê¸°ì— ë§ê²Œ ê³±í•´ì¤Œ

* cv.circle(...): ê° ëœë“œë§ˆí¬ë¥¼ ì´ˆë¡ìƒ‰ ì ìœ¼ë¡œ í”„ë ˆì„ì— í‘œì‹œ

### ESC í‚¤ë¥¼ ëˆ„ë¥´ë©´ í”„ë¡œê·¸ë¨ì´ ì¢…ë£Œë˜ë„ë¡ ì„¤ì •
```python
cv.imshow('MediaPipe FaceMesh', cv.flip(frame, 1))

if cv.waitKey(5) & 0xFF == 27:
    break

cap.release()
cv.destroyAllWindows()
```
* & 0xFF == 27: ì…ë ¥ëœ í‚¤ì˜ ASCII ì½”ë“œê°€ 27(Esc)ì¼ ê²½ìš° ì¢…ë£Œ

### ì „ì²´ ì½”ë“œ
```python
import cv2 as cv
import mediapipe as mp

# MediaPipe FaceMesh ì´ˆê¸°í™”
mp_face_mesh = mp.solutions.face_mesh
mp_drawing = mp.solutions.drawing_utils
mp_styles = mp.solutions.drawing_styles

face_mesh = mp_face_mesh.FaceMesh(
    static_image_mode=False,
    max_num_faces=1,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# ì›¹ìº ì—ì„œ ì˜ìƒ ìº¡ì²˜
cap = cv.VideoCapture(0, cv.CAP_DSHOW)

while True:
    ret, frame = cap.read()
    if not ret:
        print("í”„ë ˆì„ íšë“ ì‹¤íŒ¨")
        break

    # RGBë¡œ ë³€í™˜ í›„ ì–¼êµ´ ëœë“œë§ˆí¬ ì²˜ë¦¬
    rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)
    result = face_mesh.process(rgb_frame)

    # ì–¼êµ´ ëœë“œë§ˆí¬ê°€ ê²€ì¶œë˜ì—ˆì„ ê²½ìš°
    if result.multi_face_landmarks:
        for face_landmarks in result.multi_face_landmarks:
            # ê° ëœë“œë§ˆí¬ë¥¼ ì ìœ¼ë¡œ í‘œì‹œ
            for landmark in face_landmarks.landmark:
                x = int(landmark.x * frame.shape[1])  # ì´ë¯¸ì§€ ë„ˆë¹„ ê¸°ì¤€ xì¢Œí‘œ
                y = int(landmark.y * frame.shape[0])  # ì´ë¯¸ì§€ ë†’ì´ ê¸°ì¤€ yì¢Œí‘œ
                cv.circle(frame, (x, y), 1, (0, 255, 0), -1)  # ì  ê·¸ë¦¬ê¸° (ì´ˆë¡ìƒ‰)

    # ì¢Œìš° ë°˜ì „ í›„ ì¶œë ¥
    cv.imshow('MediaPipe FaceMesh', cv.flip(frame, 1))

    # ESC í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ (ASCII ì½”ë“œ 27)
    if cv.waitKey(5) & 0xFF == 27:
        break

cap.release()
cv.destroyAllWindows()
```

### ê²°ê³¼ í™”ë©´








